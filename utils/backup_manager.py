# utils/backup_manager.py
"""
Ù…Ø¯ÛŒØ±ÛŒØª Ø®ÙˆØ¯Ú©Ø§Ø± Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø² Ú©Ù„ Ø³ÛŒØ³ØªÙ…
- Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³
- Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø­Ø§ÙØ¸Ù‡ AI
- Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª
- ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±
- Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ
- Ø¢Ù¾Ù„ÙˆØ¯ Ø¨Ù‡ ÙØ¶Ø§ÛŒ Ø§Ø¨Ø±ÛŒ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
"""

import os
import shutil
import zipfile
import gzip
import pickle
import json
from datetime import datetime, timedelta
from pathlib import Path
import logging
import threading
import time
import schedule
from typing import Dict, List, Optional
import hashlib

logger = logging.getLogger(__name__)

class BackupManager:
    """
    Ù…Ø¯ÛŒØ±ÛŒØª Ø®ÙˆØ¯Ú©Ø§Ø± Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ
    """
    
    def __init__(self, backup_dir: str = "backups", 
                 db_path: str = "data/oracle.db",
                 memory_dir: str = "memory",
                 config_files: List[str] = None,
                 auto_backup_interval: int = 24):  # Ø³Ø§Ø¹Øª
        
        self.backup_dir = Path(backup_dir)
        self.db_path = Path(db_path)
        self.memory_dir = Path(memory_dir)
        self.config_files = config_files or ["config.py", ".env"]
        self.auto_backup_interval = auto_backup_interval
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ backup Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
        # Ø¢Ù…Ø§Ø±
        self.stats = {
            'total_backups': 0,
            'last_backup': None,
            'total_size_mb': 0,
            'backup_history': []
        }
        
        # Ù„Ø§Ú¯
        logger.info(f"ğŸ“¦ BackupManager initialized: {self.backup_dir}")
        
        # Ø´Ø±ÙˆØ¹ thread Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±
        self.start_auto_backup()
    
    def create_backup(self, backup_name: str = None) -> Dict:
        """
        Ø§ÛŒØ¬Ø§Ø¯ Ù¾Ø´ØªÛŒØ¨Ø§Ù† Ú©Ø§Ù…Ù„ Ø§Ø² Ø³ÛŒØ³ØªÙ…
        
        Args:
            backup_name: Ù†Ø§Ù… Ø¯Ù„Ø®ÙˆØ§Ù‡ Ø¨Ø±Ø§ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù† (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
        
        Returns:
            Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø´ØªÛŒØ¨Ø§Ù† Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù‡
        """
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        if not backup_name:
            backup_name = f"backup_{timestamp}"
        
        backup_file = self.backup_dir / f"{backup_name}.zip"
        
        logger.info(f"ğŸ”„ Creating backup: {backup_name}")
        
        try:
            with zipfile.ZipFile(backup_file, 'w', zipfile.ZIP_DEFLATED) as zipf:
                
                # 1. Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³
                if self.db_path.exists():
                    # Ú©Ù¾ÛŒ Ù…ÙˆÙ‚Øª Ø¯ÛŒØªØ§Ø¨ÛŒØ³ (Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù‚ÙÙ„ Ø´Ø¯Ù†)
                    temp_db = self.backup_dir / "temp_db.sqlite"
                    shutil.copy2(self.db_path, temp_db)
                    zipf.write(temp_db, "database/oracle.db")
                    temp_db.unlink()  # Ø­Ø°Ù ÙØ§ÛŒÙ„ Ù…ÙˆÙ‚Øª
                    logger.info(f"  âœ… Database backed up: {self.db_path}")
                
                # 2. Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø­Ø§ÙØ¸Ù‡ AI
                if self.memory_dir.exists():
                    for mem_file in self.memory_dir.glob("*"):
                        if mem_file.is_file():
                            zipf.write(mem_file, f"memory/{mem_file.name}")
                    logger.info(f"  âœ… AI Memory backed up: {self.memory_dir}")
                
                # 3. Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª
                for config_file in self.config_files:
                    config_path = Path(config_file)
                    if config_path.exists():
                        zipf.write(config_path, f"config/{config_path.name}")
                        logger.info(f"  âœ… Config backed up: {config_file}")
                
                # 4. Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù„Ø§Ú¯â€ŒÙ‡Ø§ (ÙÙ‚Ø· Ø¢Ø®Ø±ÛŒÙ†)
                log_dir = Path("logs")
                if log_dir.exists():
                    for log_file in sorted(log_dir.glob("*.log"), key=lambda p: p.stat().st_mtime, reverse=True)[:3]:
                        zipf.write(log_file, f"logs/{log_file.name}")
                
                # 5. Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„ Ø§Ø·Ù„Ø§Ø¹Ø§Øª
                info = {
                    'backup_name': backup_name,
                    'timestamp': timestamp,
                    'datetime': datetime.now().isoformat(),
                    'files': [str(f) for f in zipf.namelist()],
                    'size_bytes': 0,
                    'checksum': None
                }
                
                # Ù†ÙˆØ´ØªÙ† ÙØ§ÛŒÙ„ info
                zipf.writestr("backup_info.json", json.dumps(info, indent=2))
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø³Ø§ÛŒØ² Ùˆ checksum
            size_bytes = backup_file.stat().st_size
            size_mb = size_bytes / (1024 * 1024)
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ MD5 checksum
            with open(backup_file, 'rb') as f:
                md5 = hashlib.md5(f.read()).hexdigest()
            
            # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¢Ù…Ø§Ø±
            self.stats['total_backups'] += 1
            self.stats['last_backup'] = datetime.now().isoformat()
            self.stats['total_size_mb'] += size_mb
            self.stats['backup_history'].append({
                'name': backup_name,
                'time': timestamp,
                'size_mb': round(size_mb, 2),
                'md5': md5
            })
            
            # Ù†Ú¯Ù‡ Ø¯Ø§Ø´ØªÙ† ÙÙ‚Ø· Û±Û°Û° Ø±Ú©ÙˆØ±Ø¯ Ø¢Ø®Ø±
            if len(self.stats['backup_history']) > 100:
                self.stats['backup_history'] = self.stats['backup_history'][-100:]
            
            logger.info(f"âœ… Backup completed: {backup_file} ({size_mb:.2f} MB)")
            
            return {
                'success': True,
                'file': str(backup_file),
                'name': backup_name,
                'size_mb': round(size_mb, 2),
                'md5': md5,
                'files_count': len(info['files'])
            }
            
        except Exception as e:
            logger.error(f"âŒ Backup failed: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def create_ai_memory_backup(self) -> Dict:
        """
        Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø®ØªØµØ§ØµÛŒ Ø§Ø² Ø­Ø§ÙØ¸Ù‡ AI
        (Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ù‡Ø± Ø³Ø§Ø¹Øª ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯)
        """
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"ai_memory_{timestamp}"
        backup_file = self.backup_dir / f"{backup_name}.pkl.gz"
        
        try:
            if not self.memory_dir.exists():
                return {'success': False, 'error': 'Memory directory not found'}
            
            # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù‡Ù…Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø­Ø§ÙØ¸Ù‡
            memory_data = {
                'timestamp': datetime.now().isoformat(),
                'files': {}
            }
            
            for mem_file in self.memory_dir.glob("*.pkl"):
                with open(mem_file, 'rb') as f:
                    memory_data['files'][mem_file.name] = pickle.load(f)
            
            # ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø°Ø®ÛŒØ±Ù‡
            with gzip.open(backup_file, 'wb') as f:
                pickle.dump(memory_data, f)
            
            size_kb = backup_file.stat().st_size / 1024
            
            logger.info(f"ğŸ§  AI Memory backup: {backup_file} ({size_kb:.1f} KB)")
            
            return {
                'success': True,
                'file': str(backup_file),
                'size_kb': round(size_kb, 1)
            }
            
        except Exception as e:
            logger.error(f"âŒ AI Memory backup failed: {e}")
            return {'success': False, 'error': str(e)}
    
    def restore_backup(self, backup_file: str, restore_ai_memory: bool = True) -> Dict:
        """
        Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø§Ø² Ù¾Ø´ØªÛŒØ¨Ø§Ù†
        
        Args:
            backup_file: Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ù¾Ø´ØªÛŒØ¨Ø§Ù†
            restore_ai_memory: Ø¢ÛŒØ§ Ø­Ø§ÙØ¸Ù‡ AI Ù‡Ù… Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø´ÙˆØ¯ØŸ
        """
        
        backup_path = self.backup_dir / backup_file
        
        if not backup_path.exists():
            return {'success': False, 'error': 'Backup file not found'}
        
        try:
            with zipfile.ZipFile(backup_path, 'r') as zipf:
                
                # Ø¨Ø±Ø±Ø³ÛŒ ÙØ§ÛŒÙ„ info
                if 'backup_info.json' in zipf.namelist():
                    info = json.loads(zipf.read('backup_info.json'))
                
                # Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³
                if 'database/oracle.db' in zipf.namelist():
                    # Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³ ÙØ¹Ù„ÛŒ (Ù‚Ø¨Ù„ Ø§Ø² Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ)
                    current_backup = self.create_backup("before_restore")
                    
                    # Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ
                    zipf.extract('database/oracle.db', 'data/')
                    logger.info("  âœ… Database restored")
                
                # Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø­Ø§ÙØ¸Ù‡ AI
                if restore_ai_memory:
                    for mem_file in zipf.namelist():
                        if mem_file.startswith('memory/'):
                            zipf.extract(mem_file, './')
                            logger.info(f"  âœ… Restored: {mem_file}")
                
                # Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª
                for config_file in zipf.namelist():
                    if config_file.startswith('config/'):
                        zipf.extract(config_file, './')
                        logger.info(f"  âœ… Restored: {config_file}")
            
            logger.info(f"âœ… Restored from: {backup_file}")
            
            return {
                'success': True,
                'backup': backup_file,
                'timestamp': info.get('timestamp') if 'info' in locals() else None
            }
            
        except Exception as e:
            logger.error(f"âŒ Restore failed: {e}")
            return {'success': False, 'error': str(e)}
    
    def cleanup_old_backups(self, keep_days: int = 7) -> Dict:
        """
        Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ
        
        Args:
            keep_days: ØªØ¹Ø¯Ø§Ø¯ Ø±ÙˆØ²Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ù¾Ø´ØªÛŒØ¨Ø§Ù† Ù†Ú¯Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø´ÙˆØ¯
        """
        
        cutoff = datetime.now() - timedelta(days=keep_days)
        deleted = []
        kept = []
        
        for backup_file in self.backup_dir.glob("backup_*.zip"):
            mtime = datetime.fromtimestamp(backup_file.stat().st_mtime)
            if mtime < cutoff:
                size = backup_file.stat().st_size / (1024 * 1024)
                backup_file.unlink()
                deleted.append({
                    'name': backup_file.name,
                    'size_mb': round(size, 2),
                    'age_days': (datetime.now() - mtime).days
                })
                logger.info(f"ğŸ—‘ï¸ Deleted old backup: {backup_file.name}")
            else:
                kept.append(backup_file.name)
        
        # Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÙ‡Ø§ÛŒ AI Memory Ù‚Ø¯ÛŒÙ…ÛŒ
        for mem_backup in self.backup_dir.glob("ai_memory_*.pkl.gz"):
            mtime = datetime.fromtimestamp(mem_backup.stat().st_mtime)
            if mtime < cutoff:
                mem_backup.unlink()
                logger.info(f"ğŸ—‘ï¸ Deleted old AI memory: {mem_backup.name}")
        
        result = {
            'success': True,
            'deleted_count': len(deleted),
            'deleted': deleted,
            'kept_count': len(kept),
            'kept': kept[:10]  # ÙÙ‚Ø· Û±Û° ØªØ§ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´
        }
        
        logger.info(f"ğŸ§¹ Cleanup completed: {len(deleted)} backups deleted")
        
        return result
    
    def list_backups(self) -> List[Dict]:
        """Ù„ÛŒØ³Øª ØªÙ…Ø§Ù… Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯"""
        
        backups = []
        
        for backup_file in sorted(self.backup_dir.glob("backup_*.zip"), 
                                  key=lambda p: p.stat().st_mtime, 
                                  reverse=True):
            
            mtime = datetime.fromtimestamp(backup_file.stat().st_mtime)
            size_mb = backup_file.stat().st_size / (1024 * 1024)
            
            # Ø®ÙˆØ§Ù†Ø¯Ù† info Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯
            info = {}
            try:
                with zipfile.ZipFile(backup_file, 'r') as zipf:
                    if 'backup_info.json' in zipf.namelist():
                        info = json.loads(zipf.read('backup_info.json'))
            except:
                pass
            
            backups.append({
                'name': backup_file.name,
                'date': mtime.strftime('%Y-%m-%d %H:%M:%S'),
                'size_mb': round(size_mb, 2),
                'age_days': round((datetime.now() - mtime).days, 1),
                'info': info
            })
        
        return backups
    
    def start_auto_backup(self):
        """Ø´Ø±ÙˆØ¹ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ø¯Ø± thread Ø¬Ø¯Ø§"""
        
        def run_schedule():
            schedule.every(self.auto_backup_interval).hours.do(self.create_backup)
            schedule.every().hour.do(self.create_ai_memory_backup)
            schedule.every().day.at("03:00").do(self.cleanup_old_backups, keep_days=KEEP_BACKUPS_DAYS)
            
            while True:
                schedule.run_pending()
                time.sleep(60)
        
        thread = threading.Thread(target=run_schedule, daemon=True)
        thread.start()
        logger.info(f"â° Auto-backup scheduled every {self.auto_backup_interval} hours")
    
    def get_stats(self) -> Dict:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø± Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ"""
        
        return {
            'total_backups': self.stats['total_backups'],
            'last_backup': self.stats['last_backup'],
            'total_size_mb': round(self.stats['total_size_mb'], 2),
            'backup_dir': str(self.backup_dir),
            'free_space_mb': self._get_free_space(),
            'recent_backups': self.list_backups()[:5]
        }
    
    def _get_free_space(self) -> float:
        """Ø¯Ø±ÛŒØ§ÙØª ÙØ¶Ø§ÛŒ Ø®Ø§Ù„ÛŒ Ø¯ÛŒØ³Ú© (Ù…Ú¯Ø§Ø¨Ø§ÛŒØª)"""
        import shutil
        disk = shutil.disk_usage(self.backup_dir)
        return disk.free / (1024 * 1024)
