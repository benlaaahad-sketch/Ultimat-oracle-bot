# ai/ai_memory_manager.py
"""
Ù…Ø¯ÛŒØ±ÛŒØª Ø­Ø§ÙØ¸Ù‡ Ø¨Ù„Ù†Ø¯Ù…Ø¯Øª AI Ø¨Ø§ Ù¾Ø´ØªÛŒØ¨Ø§Ù† Ø®ÙˆØ¯Ú©Ø§Ø±
- Ø°Ø®ÛŒØ±Ù‡ Ø®ÙˆØ¯Ú©Ø§Ø± Ù‡Ø± Ø³Ø§Ø¹Øª
- Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø±ÙˆØ²Ø§Ù†Ù‡
- ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ
- Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø§Ø² Ù¾Ø´ØªÛŒØ¨Ø§Ù†
- Ø¢Ù…Ø§Ø± Ø­Ø§ÙØ¸Ù‡
"""

import pickle
import gzip
import json
import shutil
from datetime import datetime, timedelta
from pathlib import Path
import threading
import schedule
import time
import logging
from typing import Dict, List, Any, Optional
import hashlib
import os

logger = logging.getLogger(__name__)

class AIMemoryManager:
    """
    Ù…Ø¯ÛŒØ±ÛŒØª Ø­Ø§ÙØ¸Ù‡ AI Ø¨Ø§ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±
    """
    
    def __init__(self, memory_dir: str = "memory", 
                 backup_dir: str = "backups/ai_memory",
                 auto_save_interval: int = 60,  # Ø¯Ù‚ÛŒÙ‚Ù‡
                 compression: bool = True):
        
        self.memory_dir = Path(memory_dir)
        self.backup_dir = Path(backup_dir)
        self.auto_save_interval = auto_save_interval
        self.compression = compression
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§
        self.memory_dir.mkdir(parents=True, exist_ok=True)
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
        # Ø­Ø§ÙØ¸Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
        self.memories = {
            'patterns': [],        # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ú©Ø´Ù Ø´Ø¯Ù‡
            'learnings': [],        # ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒâ€ŒÙ‡Ø§
            'user_memories': {},    # Ø­Ø§ÙØ¸Ù‡ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†
            'predictions': [],       # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ÛŒ Ú¯Ø°Ø´ØªÙ‡
            'correlations': {},      # Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§
            'api_keys': {},          # API keyÙ‡Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ (encode Ø´Ø¯Ù‡)
            'stats': {}              # Ø¢Ù…Ø§Ø±
        }
        
        # Ù„Ø§Ú© Ø¨Ø±Ø§ÛŒ thread safety
        self.lock = threading.Lock()
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø­Ø§ÙØ¸Ù‡ Ù‚Ø¨Ù„ÛŒ
        self.load_memory()
        
        # Ø´Ø±ÙˆØ¹ thread Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±
        self.start_auto_save()
        
        logger.info(f"ğŸ§  AIMemoryManager initialized: {self.memory_dir}")
    
    # ==================== ØªÙˆØ§Ø¨Ø¹ Ø°Ø®ÛŒØ±Ù‡ Ùˆ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ====================
    
    def save_memory(self, memory_type: str = None):
        """
        Ø°Ø®ÛŒØ±Ù‡ Ø­Ø§ÙØ¸Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„
        """
        with self.lock:
            try:
                if memory_type:
                    # Ø°Ø®ÛŒØ±Ù‡ ÛŒÚ© Ù†ÙˆØ¹ Ø®Ø§Øµ
                    if memory_type in self.memories:
                        self._save_single_memory(memory_type)
                else:
                    # Ø°Ø®ÛŒØ±Ù‡ Ù‡Ù…Ù‡
                    for mem_type in self.memories:
                        self._save_single_memory(mem_type)
                
                logger.debug(f"ğŸ’¾ Memory saved: {memory_type or 'all'}")
                
            except Exception as e:
                logger.error(f"Error saving memory: {e}")
    
    def _save_single_memory(self, memory_type: str):
        """Ø°Ø®ÛŒØ±Ù‡ ÛŒÚ© Ù†ÙˆØ¹ Ø­Ø§ÙØ¸Ù‡"""
        
        file_path = self.memory_dir / f"{memory_type}.pkl"
        
        # ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ú¯Ø± Ù„Ø§Ø²Ù… Ø¨Ø§Ø´Ø¯
        if self.compression:
            file_path = Path(str(file_path) + ".gz")
            with gzip.open(file_path, 'wb') as f:
                pickle.dump(self.memories[memory_type], f)
        else:
            with open(file_path, 'wb') as f:
                pickle.dump(self.memories[memory_type], f)
    
    def load_memory(self):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø­Ø§ÙØ¸Ù‡ Ø§Ø² ÙØ§ÛŒÙ„"""
        
        with self.lock:
            for memory_type in self.memories.keys():
                self._load_single_memory(memory_type)
            
            logger.info(f"ğŸ“š Memory loaded: {len(self.memories['patterns'])} patterns, "
                       f"{len(self.memories['learnings'])} learnings, "
                       f"{len(self.memories['user_memories'])} users")
    
    def _load_single_memory(self, memory_type: str):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÛŒÚ© Ù†ÙˆØ¹ Ø­Ø§ÙØ¸Ù‡"""
        
        # Ø¨Ø±Ø±Ø³ÛŒ ÙØ§ÛŒÙ„ Ù…Ø¹Ù…ÙˆÙ„ÛŒ
        file_path = self.memory_dir / f"{memory_type}.pkl"
        if file_path.exists():
            with open(file_path, 'rb') as f:
                self.memories[memory_type] = pickle.load(f)
            return
        
        # Ø¨Ø±Ø±Ø³ÛŒ ÙØ§ÛŒÙ„ ÙØ´Ø±Ø¯Ù‡
        gz_path = self.memory_dir / f"{memory_type}.pkl.gz"
        if gz_path.exists():
            with gzip.open(gz_path, 'rb') as f:
                self.memories[memory_type] = pickle.load(f)
            return
        
        # Ø§Ú¯Ø± ÙØ§ÛŒÙ„ Ù†Ø¨ÙˆØ¯ØŒ Ù…Ù‚Ø¯Ø§Ø± Ù¾ÛŒØ´â€ŒÙØ±Ø¶
        if memory_type == 'patterns':
            self.memories[memory_type] = []
        elif memory_type == 'learnings':
            self.memories[memory_type] = []
        elif memory_type == 'user_memories':
            self.memories[memory_type] = {}
        elif memory_type == 'predictions':
            self.memories[memory_type] = []
        elif memory_type == 'correlations':
            self.memories[memory_type] = {}
        elif memory_type == 'api_keys':
            self.memories[memory_type] = {}
        elif memory_type == 'stats':
            self.memories[memory_type] = {}
    
    # ==================== ØªÙˆØ§Ø¨Ø¹ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ ====================
    
    def create_backup(self, backup_name: str = None) -> Dict:
        """
        Ø§ÛŒØ¬Ø§Ø¯ Ù¾Ø´ØªÛŒØ¨Ø§Ù† Ú©Ø§Ù…Ù„ Ø§Ø² Ø­Ø§ÙØ¸Ù‡
        """
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        if not backup_name:
            backup_name = f"ai_memory_backup_{timestamp}"
        
        backup_file = self.backup_dir / f"{backup_name}.pkl.gz"
        
        try:
            # Ø°Ø®ÛŒØ±Ù‡ Ù…ÙˆÙ‚Øª Ù‡Ù…Ù‡ Ø­Ø§ÙØ¸Ù‡â€ŒÙ‡Ø§
            self.save_memory()
            
            # Ø§ÛŒØ¬Ø§Ø¯ Ø¨Ø³ØªÙ‡ Ú©Ø§Ù…Ù„
            complete_memory = {
                'timestamp': timestamp,
                'version': '1.0',
                'memories': self.memories,
                'stats': {
                    'patterns': len(self.memories['patterns']),
                    'learnings': len(self.memories['learnings']),
                    'users': len(self.memories['user_memories']),
                    'predictions': len(self.memories['predictions'])
                }
            }
            
            # ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø°Ø®ÛŒØ±Ù‡
            with gzip.open(backup_file, 'wb') as f:
                pickle.dump(complete_memory, f)
            
            size_mb = backup_file.stat().st_size / (1024 * 1024)
            
            logger.info(f"ğŸ’¾ AI Memory backup created: {backup_file} ({size_mb:.2f} MB)")
            
            return {
                'success': True,
                'file': str(backup_file),
                'size_mb': round(size_mb, 2),
                'stats': complete_memory['stats']
            }
            
        except Exception as e:
            logger.error(f"âŒ AI Memory backup failed: {e}")
            return {'success': False, 'error': str(e)}
    
    def restore_backup(self, backup_file: str) -> Dict:
        """
        Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø­Ø§ÙØ¸Ù‡ Ø§Ø² Ù¾Ø´ØªÛŒØ¨Ø§Ù†
        """
        
        backup_path = self.backup_dir / backup_file
        if not backup_path.exists():
            return {'success': False, 'error': 'Backup file not found'}
        
        try:
            with gzip.open(backup_path, 'rb') as f:
                complete_memory = pickle.load(f)
            
            with self.lock:
                self.memories = complete_memory['memories']
                
                # Ø°Ø®ÛŒØ±Ù‡ Ø¨Ø¹Ø¯ Ø§Ø² Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ
                self.save_memory()
            
            logger.info(f"âœ… AI Memory restored from: {backup_file}")
            
            return {
                'success': True,
                'timestamp': complete_memory.get('timestamp'),
                'stats': complete_memory.get('stats')
            }
            
        except Exception as e:
            logger.error(f"âŒ Restore failed: {e}")
            return {'success': False, 'error': str(e)}
    
    def list_backups(self) -> List[Dict]:
        """Ù„ÛŒØ³Øª Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯"""
        
        backups = []
        for backup_file in sorted(self.backup_dir.glob("ai_memory_backup_*.pkl.gz"),
                                  key=lambda p: p.stat().st_mtime,
                                  reverse=True):
            
            mtime = datetime.fromtimestamp(backup_file.stat().st_mtime)
            size_mb = backup_file.stat().st_size / (1024 * 1024)
            
            backups.append({
                'name': backup_file.name,
                'date': mtime.strftime('%Y-%m-%d %H:%M:%S'),
                'size_mb': round(size_mb, 2),
                'age_days': round((datetime.now() - mtime).days, 1)
            })
        
        return backups
    
    def cleanup_old_backups(self, keep_days: int = 30):
        """Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ"""
        
        cutoff = datetime.now() - timedelta(days=keep_days)
        deleted = 0
        
        for backup_file in self.backup_dir.glob("ai_memory_backup_*.pkl.gz"):
            mtime = datetime.fromtimestamp(backup_file.stat().st_mtime)
            if mtime < cutoff:
                backup_file.unlink()
                deleted += 1
                logger.info(f"ğŸ—‘ï¸ Deleted old AI memory backup: {backup_file.name}")
        
        return {'deleted': deleted}
    
    # ==================== ØªÙˆØ§Ø¨Ø¹ Ù…Ø¯ÛŒØ±ÛŒØª Ø­Ø§ÙØ¸Ù‡ ====================
    
    def add_pattern(self, pattern: Dict):
        """Ø§ÙØ²ÙˆØ¯Ù† Ø§Ù„Ú¯ÙˆÛŒ Ø¬Ø¯ÛŒØ¯"""
        with self.lock:
            self.memories['patterns'].append({
                **pattern,
                'timestamp': datetime.now().isoformat()
            })
            # Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø§Ù†Ø¯Ø§Ø²Ù‡
            if len(self.memories['patterns']) > 10000:
                self.memories['patterns'] = self.memories['patterns'][-10000:]
    
    def add_learning(self, learning: Dict):
        """Ø§ÙØ²ÙˆØ¯Ù† ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¬Ø¯ÛŒØ¯"""
        with self.lock:
            self.memories['learnings'].append({
                **learning,
                'timestamp': datetime.now().isoformat()
            })
            if len(self.memories['learnings']) > 10000:
                self.memories['learnings'] = self.memories['learnings'][-10000:]
    
    def add_user_memory(self, user_id: int, key: str, value: Any):
        """Ø°Ø®ÛŒØ±Ù‡ Ø­Ø§ÙØ¸Ù‡ Ú©Ø§Ø±Ø¨Ø±"""
        with self.lock:
            if user_id not in self.memories['user_memories']:
                self.memories['user_memories'][user_id] = {}
            self.memories['user_memories'][user_id][key] = {
                'value': value,
                'timestamp': datetime.now().isoformat()
            }
    
    def get_user_memory(self, user_id: int, key: str = None) -> Any:
        """Ø¯Ø±ÛŒØ§ÙØª Ø­Ø§ÙØ¸Ù‡ Ú©Ø§Ø±Ø¨Ø±"""
        if user_id not in self.memories['user_memories']:
            return None if key else {}
        
        if key:
            data = self.memories['user_memories'][user_id].get(key)
            return data['value'] if data else None
        else:
            return self.memories['user_memories'][user_id]
    
    def add_prediction(self, prediction: Dict):
        """Ø°Ø®ÛŒØ±Ù‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¢ÛŒÙ†Ø¯Ù‡"""
        with self.lock:
            self.memories['predictions'].append({
                **prediction,
                'timestamp': datetime.now().isoformat()
            })
            if len(self.memories['predictions']) > 10000:
                self.memories['predictions'] = self.memories['predictions'][-10000:]
    
    def save_api_key(self, user_id: int, api_type: str, api_key: str):
        """Ø°Ø®ÛŒØ±Ù‡ API key (Ø¨Ø§ encode)"""
        with self.lock:
            if user_id not in self.memories['api_keys']:
                self.memories['api_keys'][user_id] = {}
            
            # encode Ø³Ø§Ø¯Ù‡ (Ø¯Ø± Ù†Ø³Ø®Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø² encryption Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†)
            encoded = base64.b64encode(api_key.encode()).decode()
            
            self.memories['api_keys'][user_id][api_type] = {
                'key': encoded,
                'timestamp': datetime.now().isoformat()
            }
    
    def get_api_key(self, user_id: int, api_type: str) -> Optional[str]:
        """Ø¯Ø±ÛŒØ§ÙØª API key Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡"""
        if user_id not in self.memories['api_keys']:
            return None
        
        data = self.memories['api_keys'][user_id].get(api_type)
        if data:
            # decode
            return base64.b64decode(data['key'].encode()).decode()
        return None
    
    # ==================== Ø¢Ù…Ø§Ø± ====================
    
    def get_stats(self) -> Dict:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø± Ø­Ø§ÙØ¸Ù‡"""
        
        return {
            'patterns': len(self.memories['patterns']),
            'learnings': len(self.memories['learnings']),
            'users': len(self.memories['user_memories']),
            'predictions': len(self.memories['predictions']),
            'api_keys': sum(len(keys) for keys in self.memories['api_keys'].values()),
            'backups': len(list(self.backup_dir.glob("*.pkl.gz"))),
            'memory_size_mb': self._get_memory_size()
        }
    
    def _get_memory_size(self) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø­Ø¬Ù… Ø­Ø§ÙØ¸Ù‡"""
        total = 0
        for file in self.memory_dir.glob("*"):
            total += file.stat().st_size
        return round(total / (1024 * 1024), 2)
    
    # ==================== Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± ====================
    
    def start_auto_save(self):
        """Ø´Ø±ÙˆØ¹ Ø°Ø®ÛŒØ±Ù‡ Ùˆ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±"""
        
        def run_schedule():
            # Ø°Ø®ÛŒØ±Ù‡ Ù‡Ø± 60 Ø¯Ù‚ÛŒÙ‚Ù‡
            schedule.every(self.auto_save_interval).minutes.do(self.save_memory)
            
            # Ù¾Ø´ØªÛŒØ¨Ø§Ù† Ø±ÙˆØ²Ø§Ù†Ù‡
            schedule.every().day.at("02:00").do(self.create_backup)
            
            # Ù¾Ø´ØªÛŒØ¨Ø§Ù† Ù‡ÙØªÚ¯ÛŒ Ú©Ø§Ù…Ù„
            schedule.every().monday.at("03:00").do(self.create_backup, "weekly_backup")
            
            # Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ Ù‡Ø± Ù‡ÙØªÙ‡
            schedule.every().sunday.at("04:00").do(self.cleanup_old_backups, keep_days=30)
            
            while True:
                schedule.run_pending()
                time.sleep(60)
        
        thread = threading.Thread(target=run_schedule, daemon=True)
        thread.start()
        logger.info(f"â° Auto-save scheduled every {self.auto_save_interval} minutes")
