# ai/genius_ai.py
"""
Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù†Ø§Ø¨ØºÙ‡ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª:
- ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¹Ù…ÛŒÙ‚ Ø§Ø² Ú©ØªØ§Ø¨â€ŒÙ‡Ø§
- Ø­Ø§ÙØ¸Ù‡ Ø¨Ù„Ù†Ø¯Ù…Ø¯Øª (ÛŒÚ© Ø³Ø§Ù„Ù‡)
- Ø®ÙˆØ¯Ø¢Ù…ÙˆØ²ÛŒ Ø§Ø² Ù†ØªØ§ÛŒØ¬
- ØªØ±Ú©ÛŒØ¨ Ú†Ù†Ø¯ Ù…Ø¯Ù„ (Ensemble)
- ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ùˆ Ø§Ù„Ú¯ÙˆÙ‡Ø§
- Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§ Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§
"""

import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any, Union
import json
import pickle
import hashlib
import logging
import asyncio
import aiohttp
from collections import Counter, defaultdict
import math
import random

# Machine Learning
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR, SVC
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans, DBSCAN
from sklearn.decomposition import PCA

# Deep Learning
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models, optimizers, losses, metrics
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau

# NLP
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
from textblob import TextBlob
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VaderAnalyzer
import re

# Transformers (Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø²Ø¨Ø§Ù†ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡)
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModel

# Prophet Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ
from prophet import Prophet
from prophet.serialize import model_to_json, model_from_json

# Stats
import statsmodels.api as sm
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.stattools import adfuller

# Prophet
from prophet import Prophet

# Local
from database.models import LearningMemory, Prediction, Feedback, get_db
from core.numerology_engine import NumerologyEngine
from config import *

logger = logging.getLogger(__name__)

class GeniusAI:
    """
    Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù†Ø§Ø¨ØºÙ‡ - Ù‚Ù„Ø¨ ØªÙ¾Ù†Ø¯Ù‡ Ø±Ø¨Ø§Øª
    
    Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§:
    - Û±Û° Ù…Ø¯Ù„ Ù…Ø®ØªÙ„Ù ML/DL
    - Ø­Ø§ÙØ¸Ù‡ Ø¨Ù„Ù†Ø¯Ù…Ø¯Øª (ÛŒÚ© Ø³Ø§Ù„)
    - ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø² Ú©ØªØ§Ø¨â€ŒÙ‡Ø§
    - ØªØ­Ù„ÛŒÙ„ Ú†Ù†Ø¯Ø¨Ø¹Ø¯ÛŒ
    - Ø®ÙˆØ¯Ø¨Ù‡Ø¨ÙˆØ¯ÛŒ Ù…Ø³ØªÙ…Ø±
    - Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§ Ø¨Ø§ Ensemble
    """
    
    def __init__(self, db_session=None, numerology_engine=None):
        self.db = db_session
        self.numerology = numerology_engine or NumerologyEngine(db_session)
        
        # ==================== Ø­Ø§ÙØ¸Ù‡ ====================
        self.memory = []
        self.pattern_memory = []
        self.learning_memory = []
        self.correlation_memory = {}
        
        # ==================== Ù…Ø¯Ù„â€ŒÙ‡Ø§ ====================
        self.models = {}
        self.scalers = {}
        self.vectorizers = {}
        self.ensembles = {}
        
        # ==================== NLP ====================
        self.sentiment_analyzer = VaderAnalyzer()
        self.nltk.download('vader_lexicon', quiet=True)
        
        # ==================== Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ ====================
        self.confidence_threshold = 0.65
        self.learning_rate = LEARNING_RATE
        self.memory_retention_days = MEMORY_RETENTION_DAYS
        self.auto_learn = AUTO_LEARN
        self.ensemble_voting = ENSEMBLE_VOTING
        
        # ==================== Ø¢Ù…Ø§Ø± ====================
        self.stats = {
            'total_predictions': 0,
            'correct_predictions': 0,
            'accuracy': 0.0,
            'learned_patterns': 0,
            'active_models': 0,
            'last_training': None
        }
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ùˆ Ø­Ø§ÙØ¸Ù‡
        self.load_memory()
        self.init_models()
        self.load_models()
        
        logger.info("ğŸ§  GeniusAI initialized with 10+ models and long-term memory")
    
    # ==================== Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ ====================
    
    def init_models(self):
        """Ø§ÛŒØ¬Ø§Ø¯ Û±Û° Ù…Ø¯Ù„ Ù…Ø®ØªÙ„Ù Ø¨Ø±Ø§ÛŒ Ensemble"""
        
        # 1. Random Forest
        self.models['rf_classifier'] = RandomForestClassifier(
            n_estimators=200, 
            max_depth=15,
            min_samples_split=5,
            random_state=42
        )
        self.models['rf_regressor'] = RandomForestRegressor(
            n_estimators=200,
            max_depth=15,
            random_state=42
        )
        
        # 2. Gradient Boosting
        self.models['gb_classifier'] = GradientBoostingRegressor(
            n_estimators=150,
            learning_rate=0.1,
            max_depth=5,
            random_state=42
        )
        
        # 3. SVM
        self.models['svm_classifier'] = SVC(
            kernel='rbf',
            C=1.0,
            gamma='scale',
            probability=True,
            random_state=42
        )
        self.models['svm_regressor'] = SVR(
            kernel='rbf',
            C=1.0,
            gamma='scale'
        )
        
        # 4. Neural Network (MLP)
        self.models['mlp_classifier'] = MLPRegressor(
            hidden_layer_sizes=(100, 50, 25),
            activation='relu',
            learning_rate='adaptive',
            max_iter=500,
            random_state=42
        )
        
        # 5. AdaBoost
        self.models['ada_boost'] = AdaBoostRegressor(
            n_estimators=100,
            learning_rate=0.1,
            random_state=42
        )
        
        # 6. XGBoost (Ø§Ú¯Ø± Ù†ØµØ¨ Ø¨Ø§Ø´Ù‡)
        try:
            import xgboost as xgb
            self.models['xgb_classifier'] = xgb.XGBClassifier(
                n_estimators=150,
                max_depth=6,
                learning_rate=0.1,
                random_state=42
            )
            self.models['xgb_regressor'] = xgb.XGBRegressor(
                n_estimators=150,
                max_depth=6,
                learning_rate=0.1,
                random_state=42
            )
        except ImportError:
            logger.warning("XGBoost not installed")
        
        # 7. LightGBM (Ø§Ú¯Ø± Ù†ØµØ¨ Ø¨Ø§Ø´Ù‡)
        try:
            import lightgbm as lgb
            self.models['lgb_classifier'] = lgb.LGBMClassifier(
                n_estimators=150,
                max_depth=6,
                learning_rate=0.1,
                random_state=42
            )
            self.models['lgb_regressor'] = lgb.LGBMRegressor(
                n_estimators=150,
                max_depth=6,
                learning_rate=0.1,
                random_state=42
            )
        except ImportError:
            logger.warning("LightGBM not installed")
        
        # 8. Deep Neural Network (Keras/TensorFlow)
        self.create_deep_learning_models()
        
        # 9. Prophet (Ø¨Ø±Ø§ÛŒ Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ)
        self.models['prophet'] = Prophet(
            yearly_seasonality=True,
            weekly_seasonality=True,
            daily_seasonality=True,
            changepoint_prior_scale=0.05
        )
        
        # 10. ARIMA (Ø¨Ø±Ø§ÛŒ Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ)
        # Ø¯Ø± Ø²Ù…Ø§Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯
        
        # Scalers
        self.scalers['standard'] = StandardScaler()
        self.scalers['minmax'] = MinMaxScaler()
        
        # Vectorizers
        self.vectorizers['tfidf'] = TfidfVectorizer(max_features=1000)
        
        # Ensemble meta-model
        self.ensembles['meta'] = RandomForestRegressor(n_estimators=50, random_state=42)
        
        self.stats['active_models'] = len(self.models)
    
    def create_deep_learning_models(self):
        """Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Deep Learning Ø¨Ø§ TensorFlow"""
        
        # Ù…Ø¯Ù„ Û±: Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ø¹Ù…ÛŒÙ‚ Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
        dnn_classifier = keras.Sequential([
            layers.Dense(256, activation='relu', input_shape=(100,)),
            layers.Dropout(0.3),
            layers.Dense(128, activation='relu'),
            layers.Dropout(0.2),
            layers.Dense(64, activation='relu'),
            layers.Dense(32, activation='relu'),
            layers.Dense(1, activation='sigmoid')
        ])
        
        dnn_classifier.compile(
            optimizer=optimizers.Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy', metrics.AUC()]
        )
        
        self.models['dnn_classifier'] = dnn_classifier
        
        # Ù…Ø¯Ù„ Û²: Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ø¨Ø±Ø§ÛŒ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ†
        dnn_regressor = keras.Sequential([
            layers.Dense(256, activation='relu', input_shape=(100,)),
            layers.Dropout(0.2),
            layers.Dense(128, activation='relu'),
            layers.Dense(64, activation='relu'),
            layers.Dense(32, activation='relu'),
            layers.Dense(1, activation='linear')
        ])
        
        dnn_regressor.compile(
            optimizer=optimizers.Adam(learning_rate=0.001),
            loss='mse',
            metrics=['mae']
        )
        
        self.models['dnn_regressor'] = dnn_regressor
        
        # Ù…Ø¯Ù„ Û³: LSTM Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ
        lstm_model = keras.Sequential([
            layers.LSTM(128, return_sequences=True, input_shape=(60, 10)),
            layers.LSTM(64, return_sequences=True),
            layers.LSTM(32),
            layers.Dense(16, activation='relu'),
            layers.Dense(1)
        ])
        
        lstm_model.compile(
            optimizer=optimizers.Adam(learning_rate=0.001),
            loss='mse',
            metrics=['mae']
        )
        
        self.models['lstm'] = lstm_model
        
        # Ù…Ø¯Ù„ Û´: CNN Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯Ùˆ
        cnn_model = keras.Sequential([
            layers.Conv1D(64, 3, activation='relu', input_shape=(100, 1)),
            layers.MaxPooling1D(2),
            layers.Conv1D(128, 3, activation='relu'),
            layers.GlobalAveragePooling1D(),
            layers.Dense(64, activation='relu'),
            layers.Dense(1)
        ])
        
        cnn_model.compile(
            optimizer=optimizers.Adam(learning_rate=0.001),
            loss='mse'
        )
        
        self.models['cnn'] = cnn_model
    
    # ==================== Ø­Ø§ÙØ¸Ù‡ Ø¨Ù„Ù†Ø¯Ù…Ø¯Øª ====================
    
    def load_memory(self):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø­Ø§ÙØ¸Ù‡ Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ùˆ ÙØ§ÛŒÙ„"""
        try:
            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³
            if self.db:
                memories = self.db.query(LearningMemory).filter(
                    LearningMemory.last_seen >= datetime.utcnow() - timedelta(days=self.memory_retention_days)
                ).all()
                
                for mem in memories:
                    self.learning_memory.append({
                        'pattern': mem.input_pattern,
                        'output': mem.output_pattern,
                        'confidence': mem.confidence,
                        'success_rate': mem.success_rate,
                        'occurrences': mem.occurrences,
                        'last_seen': mem.last_seen
                    })
            
            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø§Ø² ÙØ§ÛŒÙ„
            memory_file = MEMORY_DIR / 'ai_memory.pkl'
            if memory_file.exists():
                with open(memory_file, 'rb') as f:
                    saved_memory = pickle.load(f)
                    self.pattern_memory = saved_memory.get('patterns', [])
                    self.correlation_memory = saved_memory.get('correlations', {})
                    self.stats = saved_memory.get('stats', self.stats)
            
            logger.info(f"ğŸ“š Loaded {len(self.learning_memory)} memories, {len(self.pattern_memory)} patterns")
            
        except Exception as e:
            logger.error(f"Error loading memory: {e}")
    
    def save_memory(self):
        """Ø°Ø®ÛŒØ±Ù‡ Ø­Ø§ÙØ¸Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„"""
        try:
            memory_file = MEMORY_DIR / 'ai_memory.pkl'
            with open(memory_file, 'wb') as f:
                pickle.dump({
                    'patterns': self.pattern_memory[-10000:],  # Ù†Ú¯Ù‡â€ŒØ¯Ø§Ø±ÛŒ 10000 Ø§Ù„Ú¯ÙˆÛŒ Ø¢Ø®Ø±
                    'correlations': self.correlation_memory,
                    'stats': self.stats
                }, f)
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
            if self.db:
                for mem in self.learning_memory[-100:]:  # 100 ØªØ§ÛŒ Ø¢Ø®Ø±
                    db_mem = LearningMemory(
                        memory_type='pattern',
                        input_pattern=mem['pattern'],
                        output_pattern=mem['output'],
                        confidence=mem.get('confidence', 0.5),
                        occurrences=mem.get('occurrences', 1),
                        success_rate=mem.get('success_rate', 0.5),
                        last_seen=datetime.utcnow()
                    )
                    self.db.add(db_mem)
                
                self.db.commit()
            
            logger.info("ğŸ’¾ Memory saved successfully")
            
        except Exception as e:
            logger.error(f"Error saving memory: {e}")
    
    def learn_from_experience(self, prediction_data: Dict, actual_outcome: Any):
        """ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø² Ù†ØªØ§ÛŒØ¬ ÙˆØ§Ù‚Ø¹ÛŒ"""
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
        features = self.extract_features(prediction_data)
        success = self.evaluate_success(prediction_data, actual_outcome)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡
        memory_item = {
            'pattern': features,
            'output': actual_outcome,
            'success': success,
            'timestamp': datetime.utcnow(),
            'prediction_id': prediction_data.get('id')
        }
        
        self.learning_memory.append(memory_item)
        
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ
        self.find_patterns()
        
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¢Ù…Ø§Ø±
        self.stats['total_predictions'] += 1
        if success:
            self.stats['correct_predictions'] += 1
        
        self.stats['accuracy'] = self.stats['correct_predictions'] / max(1, self.stats['total_predictions'])
        
        # Ø¨Ø§Ø²Ø¢Ù…ÙˆØ²ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§
        if self.auto_learn and len(self.learning_memory) % 100 == 0:
            asyncio.create_task(self.retrain_models())
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø­Ø§ÙØ¸Ù‡
        if len(self.learning_memory) % 1000 == 0:
            self.save_memory()
    
    def find_patterns(self):
        """Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡"""
        
        if len(self.learning_memory) < 10:
            return
        
        # Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø´Ø§Ø¨Ù‡
        from sklearn.cluster import DBSCAN
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ feature vectors
        X = []
        for mem in self.learning_memory[-1000:]:  # 1000 ØªØ§ÛŒ Ø¢Ø®Ø±
            if 'pattern' in mem and mem['pattern']:
                # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ vector
                vec = self._dict_to_vector(mem['pattern'])
                X.append(vec)
        
        if len(X) < 10:
            return
        
        X = np.array(X)
        
        # Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
        clustering = DBSCAN(eps=0.5, min_samples=3).fit(X)
        
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ù†ÛŒâ€ŒØ¯Ø§Ø±
        unique_labels = set(clustering.labels_)
        
        for label in unique_labels:
            if label == -1:  # Ù†ÙˆÛŒØ²
                continue
            
            cluster_mask = clustering.labels_ == label
            cluster_size = np.sum(cluster_mask)
            
            if cluster_size >= 3:  # Ø­Ø¯Ø§Ù‚Ù„ Û³ Ù†Ù…ÙˆÙ†Ù‡
                cluster_data = [self.learning_memory[-1000:][i] for i in range(len(cluster_mask)) if cluster_mask[i]]
                
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†Ø±Ø® Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø± Ø®ÙˆØ´Ù‡
                success_rate = np.mean([d.get('success', False) for d in cluster_data])
                
                if success_rate > 0.7:  # Ø§Ù„Ú¯ÙˆÛŒ Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªÙ…Ø§Ø¯
                    pattern_key = hashlib.md5(str(X[cluster_mask][0]).encode()).hexdigest()
                    
                    self.pattern_memory.append({
                        'pattern_id': pattern_key,
                        'features': X[cluster_mask][0].tolist(),
                        'success_rate': success_rate,
                        'occurrences': cluster_size,
                        'first_seen': min(d.get('timestamp', datetime.utcnow()) for d in cluster_data),
                        'last_seen': max(d.get('timestamp', datetime.utcnow()) for d in cluster_data)
                    })
        
        self.stats['learned_patterns'] = len(self.pattern_memory)
    
    # ==================== Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§ Ensemble ====================
    
    async def predict(self, input_data: Dict, prediction_type: str = 'general') -> Dict[str, Any]:
        """
        Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§ ØªØ±Ú©ÛŒØ¨ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§
        
        Args:
            input_data: Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ
            prediction_type: Ù†ÙˆØ¹ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ (crypto, sports, event, etc)
        
        Returns:
            Ù†ØªÛŒØ¬Ù‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§ Ø¬Ø²Ø¦ÛŒØ§Øª Ú©Ø§Ù…Ù„
        """
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
        features = self.extract_features(input_data)
        feature_vector = self._dict_to_vector(features)
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…Ø´Ø§Ø¨Ù‡ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡
        memory_prediction = self.check_memory_patterns(feature_vector)
        
        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§ Ø¹Ø¯Ø¯Ø´Ù†Ø§Ø³ÛŒ
        numerology_prediction = self.get_numerology_prediction(input_data, prediction_type)
        
        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ML
        ml_predictions = await self.get_ml_predictions(feature_vector, prediction_type)
        
        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§ Deep Learning
        dl_predictions = self.get_deep_learning_predictions(feature_vector)
        
        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§ Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ (Ø§Ú¯Ø±é€‚ç”¨ Ø¨Ø§Ø´Ù‡)
        time_series_prediction = await self.get_time_series_prediction(input_data)
        
        # ØªØ±Ú©ÛŒØ¨ Ù‡Ù…Ù‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ (Ensemble)
        ensemble_result = self.ensemble_predictions([
            ('memory', memory_prediction),
            ('numerology', numerology_prediction),
            ('ml', ml_predictions),
            ('dl', dl_predictions),
            ('timeseries', time_series_prediction)
        ])
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¶Ø±ÛŒØ¨ Ø§Ø·Ù…ÛŒÙ†Ø§Ù†
        confidence = self.calculate_confidence(ensemble_result)
        
        # ØªÙØ³ÛŒØ± Ù†ØªÛŒØ¬Ù‡
        interpretation = self.generate_interpretation(ensemble_result, input_data)
        
        result = {
            'prediction': ensemble_result['value'],
            'probability': ensemble_result['probability'],
            'confidence': confidence,
            'confidence_level': self.get_confidence_level(confidence),
            'recommendation': self.generate_recommendation(ensemble_result),
            'ensemble_details': ensemble_result['details'],
            'numerology_component': numerology_prediction,
            'ml_component': ml_predictions,
            'memory_component': memory_prediction,
            'interpretation': interpretation,
            'timestamp': datetime.utcnow().isoformat()
        }
        
        return result
    
    def extract_features(self, input_data: Dict) -> Dict[str, float]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ"""
        features = {}
        
        # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡
        if 'token_address' in input_data:
            addr = input_data['token_address']
            features['addr_length'] = len(addr)
            features['addr_digits'] = sum(c.isdigit() for c in addr)
            features['addr_hex_sum'] = sum(int(c, 16) if c.isdigit() else ord(c) % 16 for c in addr if c.isalnum())
        
        if 'price' in input_data:
            features['price'] = float(input_data['price'])
        
        if 'volume' in input_data:
            features['volume'] = float(input_data['volume'])
        
        if 'market_cap' in input_data:
            features['market_cap'] = float(input_data['market_cap'])
        
        # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ
        now = datetime.now()
        features['hour'] = now.hour
        features['day'] = now.day
        features['month'] = now.month
        features['year'] = now.year
        features['weekday'] = now.weekday()
        
        # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯Ø´Ù†Ø§Ø³ÛŒ
        if 'name' in input_data:
            name_num = self.numerology.calculate_name_number(input_data['name'])
            features['name_number'] = name_num.get('expression', 0)
        
        if 'birth_date' in input_data:
            life_path = self.numerology.calculate_life_path(input_data['birth_date'])
            features['life_path'] = life_path['primary_number']
        
        return features
    
    def _dict_to_vector(self, d: Dict) -> np.ndarray:
        """ØªØ¨Ø¯ÛŒÙ„ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ù‡ Ø¨Ø±Ø¯Ø§Ø± Ø¹Ø¯Ø¯ÛŒ"""
        # Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¨Ø±Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ missing
        default_values = {
            'addr_length': 42,
            'addr_digits': 10,
            'addr_hex_sum': 100,
            'price': 0,
            'volume': 0,
            'market_cap': 0,
            'hour': 0,
            'day': 1,
            'month': 1,
            'year': 2024,
            'weekday': 0,
            'name_number': 5,
            'life_path': 5
        }
        
        vector = []
        for key, default in default_values.items():
            vector.append(float(d.get(key, default)))
        
        return np.array(vector)
    
    def check_memory_patterns(self, feature_vector: np.ndarray) -> Dict[str, Any]:
        """Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…Ø´Ø§Ø¨Ù‡ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡"""
        if not self.pattern_memory:
            return {'exists': False, 'prediction': None, 'confidence': 0}
        
        best_match = None
        best_similarity = 0
        
        for pattern in self.pattern_memory[-1000:]:  # 1000 Ø§Ù„Ú¯ÙˆÛŒ Ø¢Ø®Ø±
            pattern_vec = np.array(pattern['features'])
            similarity = 1 - np.linalg.norm(feature_vector - pattern_vec) / np.linalg.norm(pattern_vec)
            
            if similarity > best_similarity and similarity > 0.8:  # Ø´Ø¨Ø§Ù‡Øª Ø¨ÛŒØ´ Ø§Ø² 80%
                best_similarity = similarity
                best_match = pattern
        
        if best_match:
            return {
                'exists': True,
                'prediction': best_match.get('success_rate', 0.5),
                'confidence': best_similarity,
                'pattern_id': best_match.get('pattern_id'),
                'occurrences': best_match.get('occurrences', 1)
            }
        
        return {'exists': False, 'prediction': None, 'confidence': 0}
    
    def get_numerology_prediction(self, input_data: Dict, pred_type: str) -> Dict[str, Any]:
        """Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¹Ø¯Ø¯Ø´Ù†Ø§Ø³ÛŒ"""
        
        if pred_type == 'crypto':
            # ØªØ­Ù„ÛŒÙ„ Ø¹Ø¯Ø¯ÛŒ Ø¢Ø¯Ø±Ø³ ØªÙˆÚ©Ù†
            if 'token_address' in input_data:
                addr_analysis = self.numerology.analyze_token_address(input_data['token_address'])
                number = addr_analysis.get('reduced_number', 5)
                
                # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¹Ø¯Ø¯
                if number in [8, 9]:
                    score = 0.8  # Ø§Ø¹Ø¯Ø§Ø¯ Ù‚Ø¯Ø±Øª
                elif number in [3, 6]:
                    score = 0.6  # Ø§Ø¹Ø¯Ø§Ø¯ ØªØ¹Ø§Ø¯Ù„
                elif number in [4, 5]:
                    score = 0.5  # Ø§Ø¹Ø¯Ø§Ø¯ Ù…ÛŒØ§Ù†Ù‡
                else:
                    score = 0.4
                
                return {
                    'value': score,
                    'number': number,
                    'analysis': addr_analysis,
                    'interpretation': f"Token number {number}: {addr_analysis.get('interpretation', '')}"
                }
        
        elif pred_type == 'sports':
            # ØªØ­Ù„ÛŒÙ„ Ø¹Ø¯Ø¯ÛŒ Ù†Ø§Ù… ØªÛŒÙ…â€ŒÙ‡Ø§
            if 'team1' in input_data and 'team2' in input_data:
                team1_num = self.numerology.calculate_name_number(input_data['team1'])
                team2_num = self.numerology.calculate_name_number(input_data['team2'])
                
                n1 = team1_num.get('expression', 5)
                n2 = team2_num.get('expression', 5)
                
                # Ù…Ù‚Ø§ÛŒØ³Ù‡
                if n1 > n2:
                    score = 0.6 + (n1 - n2) / 20
                elif n2 > n1:
                    score = 0.4 - (n2 - n1) / 20
                else:
                    score = 0.5
                
                return {
                    'value': min(max(score, 0), 1),
                    'team1_number': n1,
                    'team2_number': n2,
                    'interpretation': f"Team1: {n1}, Team2: {n2}"
                }
        
        elif pred_type == 'event':
            # ØªØ­Ù„ÛŒÙ„ Ø¹Ø¯Ø¯ÛŒ ØªØ§Ø±ÛŒØ® Ø±ÙˆÛŒØ¯Ø§Ø¯
            if 'event_date' in input_data:
                date_num = self.numerology.calculate_life_path(input_data['event_date'])
                number = date_num['primary_number']
                
                # Ø§Ø¹Ø¯Ø§Ø¯ Ø®ÙˆØ´â€ŒÛŒÙ…Ù† Ø¨Ø±Ø§ÛŒ Ø±ÙˆÛŒØ¯Ø§Ø¯
                lucky_events = [1, 3, 6, 8, 9]
                score = 0.7 if number in lucky_events else 0.5
                
                return {
                    'value': score,
                    'number': number,
                    'interpretation': f"Event date number {number}"
                }
        
        # Ù¾ÛŒØ´â€ŒÙØ±Ø¶
        return {'value': 0.5, 'interpretation': 'Neutral numerology'}
    
    async def get_ml_predictions(self, feature_vector: np.ndarray, pred_type: str) -> Dict[str, Any]:
        """Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Machine Learning"""
        
        predictions = {}
        weights = {}
        
        feature_vector = feature_vector.reshape(1, -1)
        
        for name, model in self.models.items():
            if name in ['dnn_classifier', 'dnn_regressor', 'lstm', 'cnn', 'prophet']:
                continue  # Ø§ÛŒÙ†Ù‡Ø§ deep learning Ù‡Ø³ØªÙ†Ø¯
            
            try:
                if hasattr(model, 'predict_proba'):
                    pred = model.predict_proba(feature_vector)[0][1]
                elif hasattr(model, 'predict'):
                    pred = model.predict(feature_vector)[0]
                else:
                    continue
                
                # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¨Ù‡ Ø¨Ø§Ø²Ù‡ 0-1
                if isinstance(pred, np.ndarray):
                    pred = float(pred[0]) if len(pred) > 0 else 0.5
                else:
                    pred = float(pred)
                
                pred = max(0, min(1, pred))
                
                predictions[name] = pred
                
                # ÙˆØ²Ù† Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ù‚Øª Ù‚Ø¨Ù„ÛŒ
                weights[name] = self.get_model_weight(name)
                
            except Exception as e:
                logger.debug(f"Model {name} prediction failed: {e}")
                continue
        
        if not predictions:
            return {'value': 0.5, 'models_used': 0}
        
        # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ÙˆØ²Ù†ÛŒ
        weighted_sum = sum(predictions[n] * weights.get(n, 1) for n in predictions)
        total_weight = sum(weights.get(n, 1) for n in predictions)
        
        ensemble_value = weighted_sum / total_weight if total_weight > 0 else np.mean(list(predictions.values()))
        
        return {
            'value': float(ensemble_value),
            'models_used': len(predictions),
            'individual': predictions,
            'variance': float(np.var(list(predictions.values())))
        }
    
    def get_deep_learning_predictions(self, feature_vector: np.ndarray) -> Dict[str, Any]:
        """Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Deep Learning"""
        
        predictions = {}
        
        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ DNN
        X_dnn = feature_vector.reshape(1, -1)
        
        # DNN Classifier
        if 'dnn_classifier' in self.models:
            try:
                pred = self.models['dnn_classifier'].predict(X_dnn)[0][0]
                predictions['dnn'] = float(pred)
            except:
                pass
        
        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ LSTM (Ù†ÛŒØ§Ø² Ø¨Ù‡ reshape Ù…ØªÙØ§ÙˆØª)
        if 'lstm' in self.models and len(feature_vector[0]) >= 60:
            try:
                X_lstm = feature_vector.reshape(1, 60, -1)
                pred = self.models['lstm'].predict(X_lstm)[0][0]
                predictions['lstm'] = float(pred)
            except:
                pass
        
        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ CNN
        if 'cnn' in self.models:
            try:
                X_cnn = feature_vector.reshape(1, -1, 1)
                pred = self.models['cnn'].predict(X_cnn)[0][0]
                predictions['cnn'] = float(pred)
            except:
                pass
        
        if not predictions:
            return {'value': 0.5, 'models_used': 0}
        
        return {
            'value': float(np.mean(list(predictions.values()))),
            'models_used': len(predictions),
            'individual': predictions
        }
    
    async def get_time_series_prediction(self, input_data: Dict) -> Dict[str, Any]:
        """Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ"""
        
        # Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø¨Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ù†ÛŒØ§Ø² Ø¯Ø§Ø±Ø¯
        # Ø¯Ø± Ø§ÛŒÙ† Ù†Ø³Ø®Ù‡ Ø³Ø§Ø¯Ù‡ Ø´Ø¯Ù‡
        return {'value': 0.5, 'method': 'not_available'}
    
    def ensemble_predictions(self, predictions: List[Tuple[str, Dict]]) -> Dict[str, Any]:
        """ØªØ±Ú©ÛŒØ¨ Ú†Ù†Ø¯ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§ Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù"""
        
        valid_predictions = []
        weights = []
        details = {}
        
        # ÙˆØ²Ù†â€ŒØ¯Ù‡ÛŒ Ø¨Ù‡ Ù‡Ø± Ù…Ù†Ø¨Ø¹
        weight_map = {
            'memory': 0.3,
            'numerology': 0.25,
            'ml': 0.25,
            'dl': 0.15,
            'timeseries': 0.05
        }
        
        for source, pred in predictions:
            if pred and 'value' in pred:
                value = pred['value']
                if isinstance(value, (int, float)) and 0 <= value <= 1:
                    valid_predictions.append(value)
                    weights.append(weight_map.get(source, 0.2))
                    details[source] = {
                        'value': value,
                        'weight': weight_map.get(source, 0.2),
                        'details': {k: v for k, v in pred.items() if k != 'value'}
                    }
        
        if not valid_predictions:
            return {
                'value': 0.5,
                'probability': 0.5,
                'details': {},
                'method': 'default'
            }
        
        # Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù ØªØ±Ú©ÛŒØ¨
        weights = np.array(weights) / sum(weights)
        
        # 1. Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ÙˆØ²Ù†ÛŒ
        weighted_avg = np.average(valid_predictions, weights=weights)
        
        # 2. Ù…ÛŒØ§Ù†Ù‡
        median = np.median(valid_predictions)
        
        # 3. ØªØ±Ú©ÛŒØ¨
        if len(valid_predictions) >= 3:
            # Ø­Ø°Ù Ù¾Ø±Øªâ€ŒÙ‡Ø§
            q1, q3 = np.percentile(valid_predictions, [25, 75])
            iqr = q3 - q1
            filtered = [p for p in valid_predictions if q1 - 1.5*iqr <= p <= q3 + 1.5*iqr]
            trimmed_mean = np.mean(filtered) if filtered else weighted_avg
        else:
            trimmed_mean = weighted_avg
        
        # Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† Ø±ÙˆØ´
        if np.std(valid_predictions) < 0.1:
            # Ø§Ø¬Ù…Ø§Ø¹ Ø¨Ø§Ù„Ø§ - Ø§Ø² Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ÙˆØ²Ù†ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
            final = weighted_avg
            method = 'weighted_avg (high consensus)'
        else:
            # Ø§Ø®ØªÙ„Ø§Ù Ø¨Ø§Ù„Ø§ - Ø§Ø² trimmed mean Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
            final = trimmed_mean
            method = 'trimmed_mean (low consensus)'
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø­ØªÙ…Ø§Ù„ (Ø¨Ø±Ø§ÛŒ binary predictions)
        probability = final
        
        return {
            'value': float(final),
            'probability': float(probability),
            'method': method,
            'consensus': float(1 - np.std(valid_predictions)),
            'details': details,
            'all_predictions': valid_predictions
        }
    
    def calculate_confidence(self, ensemble_result: Dict) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¶Ø±ÛŒØ¨ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ"""
        
        confidence = 0.5  # Ù¾Ø§ÛŒÙ‡
        
        # ÙØ§Ú©ØªÙˆØ± Û±: Ø§Ø¬Ù…Ø§Ø¹ Ø¨ÛŒÙ† Ù…Ø¯Ù„â€ŒÙ‡Ø§
        if 'consensus' in ensemble_result:
            confidence += ensemble_result['consensus'] * 0.3
        
        # ÙØ§Ú©ØªÙˆØ± Û²: ØªØ¹Ø¯Ø§Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆÙÙ‚
        details = ensemble_result.get('details', {})
        num_sources = len(details)
        confidence += min(num_sources * 0.05, 0.2)
        
        # ÙØ§Ú©ØªÙˆØ± Û³: ÙˆØ¬ÙˆØ¯ Ø§Ù„Ú¯Ùˆ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡
        if 'memory' in details and details['memory'].get('exists', False):
            confidence += 0.15
        
        # ÙØ§Ú©ØªÙˆØ± Û´: ØªØ·Ø§Ø¨Ù‚ Ø¨Ø§ Ø¹Ø¯Ø¯Ø´Ù†Ø§Ø³ÛŒ
        if 'numerology' in details:
            confidence += 0.05
        
        # Ù…Ø­Ø¯ÙˆØ¯ Ø¨Ù‡ 0-1
        return min(max(confidence, 0), 1)
    
    def get_confidence_level(self, confidence: float) -> str:
        """ØªØ¨Ø¯ÛŒÙ„ Ø¹Ø¯Ø¯ Ø¨Ù‡ Ø³Ø·Ø­ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ù…ØªÙ†ÛŒ"""
        if confidence >= 0.9:
            return "ğŸ”® Absolute Certainty"
        elif confidence >= 0.8:
            return "âœ¨ Very High Confidence"
        elif confidence >= 0.7:
            return "â­ High Confidence"
        elif confidence >= 0.6:
            return "ğŸ“Š Moderate Confidence"
        elif confidence >= 0.5:
            return "ğŸ“ˆ Slight Edge"
        else:
            return "âš ï¸ Low Confidence"
    
    def generate_recommendation(self, ensemble_result: Dict) -> str:
        """ØªÙˆÙ„ÛŒØ¯ ØªÙˆØµÛŒÙ‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ"""
        
        value = ensemble_result.get('value', 0.5)
        
        if value >= 0.8:
            return "ğŸš€ STRONG BUY - Exceptional opportunity"
        elif value >= 0.7:
            return "ğŸ“ˆ BUY - Positive outlook"
        elif value >= 0.6:
            return "ğŸ‘€ WATCH - Monitor closely"
        elif value >= 0.4:
            return "â¸ï¸ HOLD - Wait for clearer signals"
        else:
            return "ğŸ›‘ AVOID - Negative indicators"
    
    def generate_interpretation(self, ensemble_result: Dict, input_data: Dict) -> str:
        """ØªÙˆÙ„ÛŒØ¯ ØªÙØ³ÛŒØ± Ø§Ù†Ø³Ø§Ù†ÛŒ Ø§Ø² Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ"""
        
        value = ensemble_result.get('value', 0.5)
        confidence = ensemble_result.get('confidence', 0.5)
        
        interpretations = []
        
        # ØªÙØ³ÛŒØ± Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ù‚Ø¯Ø§Ø±
        if value >= 0.8:
            interpretations.append("The cosmic alignment is exceptionally favorable.")
        elif value >= 0.7:
            interpretations.append("The stars are aligned in your favor.")
        elif value >= 0.6:
            interpretations.append("There is positive energy surrounding this.")
        elif value >= 0.4:
            interpretations.append("The outcome is balanced - neither strongly favorable nor unfavorable.")
        else:
            interpretations.append("The cosmic energies suggest caution.")
        
        # ØªÙØ³ÛŒØ± Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø·Ù…ÛŒÙ†Ø§Ù†
        if confidence >= 0.8:
            interpretations.append("The signs are clear and unambiguous.")
        elif confidence >= 0.6:
            interpretations.append("Multiple indicators point in the same direction.")
        else:
            interpretations.append("The signals are mixed - trust your intuition.")
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¹Ø¯Ø¯Ø´Ù†Ø§Ø³ÛŒ
        if 'numerology' in ensemble_result.get('details', {}):
            num_data = ensemble_result['details']['numerology']
            if 'interpretation' in num_data:
                interpretations.append(f"Numerology reveals: {num_data['interpretation']}")
        
        return " ".join(interpretations)
    
    def get_model_weight(self, model_name: str) -> float:
        """Ø¯Ø±ÛŒØ§ÙØª ÙˆØ²Ù† Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù‚Ø¨Ù„ÛŒ"""
        # Ø§ÛŒÙ†Ø¬Ø§ Ù…ÛŒâ€ŒØªÙˆÙ†Ù‡ Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨ÛŒØ§Ø¯
        base_weights = {
            'rf_classifier': 1.2,
            'gb_classifier': 1.1,
            'svm_classifier': 1.0,
            'mlp_classifier': 0.9,
            'xgb_classifier': 1.3,
            'lgb_classifier': 1.2,
            'ada_boost': 0.8
        }
        return base_weights.get(model_name, 1.0)
    
    # ==================== Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ ====================
    
    async def retrain_models(self):
        """Ø¨Ø§Ø²Ø¢Ù…ÙˆØ²ÛŒ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯"""
        
        logger.info("ğŸ”„ Starting model retraining...")
        
        if len(self.learning_memory) < 100:
            logger.info("Not enough data for retraining")
            return
        
        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        X, y = self.prepare_training_data()
        
        if len(X) < 50:
            return
        
        # ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # Ø¢Ù…ÙˆØ²Ø´ Ù‡Ø± Ù…Ø¯Ù„
        for name, model in self.models.items():
            if name in ['prophet', 'lstm', 'cnn']:
                continue  # Ø§ÛŒÙ†Ù‡Ø§ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¯Ø§Ø¯Ù‡ Ø®Ø§Øµ Ø¯Ø§Ø±Ù†Ø¯
            
            try:
                if hasattr(model, 'fit'):
                    model.fit(X_train, y_train)
                    
                    # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
                    if hasattr(model, 'score'):
                        train_score = model.score(X_train, y_train)
                        test_score = model.score(X_test, y_test)
                        
                        logger.info(f"âœ… {name}: train={train_score:.3f}, test={test_score:.3f}")
                        
                        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ÙˆØ²Ù†
                        self.update_model_weight(name, test_score)
                        
            except Exception as e:
                logger.error(f"Error training {name}: {e}")
        
        # Ø¢Ù…ÙˆØ²Ø´ meta-model Ø¨Ø±Ø§ÛŒ ensemble
        self.train_ensemble_meta(X_train, y_train, X_test, y_test)
        
        # Ø¢Ù…ÙˆØ²Ø´ Deep Learning models
        self.train_deep_learning(X_train, y_train, X_test, y_test)
        
        self.stats['last_training'] = datetime.utcnow()
        logger.info("âœ… Model retraining completed")
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§
        self.save_models()
    
    def prepare_training_data(self) -> Tuple[np.ndarray, np.ndarray]:
        """Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ"""
        
        X = []
        y = []
        
        for mem in self.learning_memory[-5000:]:  # 5000 ØªØ§ÛŒ Ø¢Ø®Ø±
            if 'pattern' in mem and 'success' in mem:
                vec = self._dict_to_vector(mem['pattern'])
                X.append(vec)
                y.append(1.0 if mem['success'] else 0.0)
        
        return np.array(X), np.array(y)
    
    def update_model_weight(self, model_name: str, score: float):
        """Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ÙˆØ²Ù† Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¹Ù…Ù„Ú©Ø±Ø¯"""
        # Ø§ÛŒÙ†Ø¬Ø§ Ù…ÛŒâ€ŒØªÙˆÙ†Ù‡ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø°Ø®ÛŒØ±Ù‡ Ú©Ù†Ù‡
        pass
    
    def train_ensemble_meta(self, X_train, y_train, X_test, y_test):
        """Ø¢Ù…ÙˆØ²Ø´ meta-model Ø¨Ø±Ø§ÛŒ ØªØ±Ú©ÛŒØ¨ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§"""
        
        # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ÛŒ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§
        meta_features_train = []
        
        for i in range(len(X_train)):
            row_preds = []
            for name, model in self.models.items():
                if hasattr(model, 'predict'):
                    try:
                        pred = model.predict([X_train[i]])[0]
                        row_preds.append(pred)
                    except:
                        row_preds.append(0.5)
            
            if len(row_preds) == len([m for m in self.models if hasattr(m, 'predict')]):
                meta_features_train.append(row_preds)
        
        if len(meta_features_train) > 10:
            meta_features_train = np.array(meta_features_train)
            y_train_meta = y_train[:len(meta_features_train)]
            
            self.ensembles['meta'].fit(meta_features_train, y_train_meta)
            logger.info("âœ… Meta-ensemble trained")
    
    def train_deep_learning(self, X_train, y_train, X_test, y_test):
        """Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Deep Learning"""
        
        # DNN Classifier
        if 'dnn_classifier' in self.models:
            early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
            
            history = self.models['dnn_classifier'].fit(
                X_train, y_train,
                validation_data=(X_test, y_test),
                epochs=50,
                batch_size=32,
                callbacks=[early_stop],
                verbose=0
            )
            
            logger.info(f"âœ… DNN trained: loss={history.history['loss'][-1]:.4f}")
        
        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ LSTM
        if 'lstm' in self.models and len(X_train) >= 60:
            # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ÙØ±Ù…Øª LSTM
            X_lstm_train = X_train.reshape(-1, 60, X_train.shape[1] // 60)
            X_lstm_test = X_test.reshape(-1, 60, X_test.shape[1] // 60)
            
            self.models['lstm'].fit(
                X_lstm_train, y_train[:len(X_lstm_train)],
                validation_data=(X_lstm_test, y_test[:len(X_lstm_test)]),
                epochs=30,
                batch_size=16,
                verbose=0
            )
            
            logger.info("âœ… LSTM trained")
        
        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ CNN
        if 'cnn' in self.models:
            X_cnn_train = X_train.reshape(-1, X_train.shape[1], 1)
            X_cnn_test = X_test.reshape(-1, X_test.shape[1], 1)
            
            self.models['cnn'].fit(
                X_cnn_train, y_train,
                validation_data=(X_cnn_test, y_test),
                epochs=30,
                batch_size=32,
                verbose=0
            )
            
            logger.info("âœ… CNN trained")
    
    # ==================== ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª ====================
    
    def analyze_sentiment(self, text: str) -> Dict[str, float]:
        """ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù…ØªÙ† Ø¨Ø§ Û³ Ø±ÙˆØ´ Ù…Ø®ØªÙ„Ù"""
        
        results = {}
        
        # Ø±ÙˆØ´ Û±: VADER
        vader_scores = self.sentiment_analyzer.polarity_scores(text)
        results['vader'] = {
            'positive': vader_scores['pos'],
            'negative': vader_scores['neg'],
            'neutral': vader_scores['neu'],
            'compound': vader_scores['compound']
        }
        
        # Ø±ÙˆØ´ Û²: TextBlob
        blob = TextBlob(text)
        results['textblob'] = {
            'polarity': blob.sentiment.polarity,
            'subjectivity': blob.sentiment.subjectivity
        }
        
        # Ø±ÙˆØ´ Û³: ØªØ±Ú©ÛŒØ¨
        combined = (vader_scores['compound'] + blob.sentiment.polarity) / 2
        results['combined'] = combined
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¨Ù‡ 0-1
        results['normalized'] = (combined + 1) / 2
        
        return results
    
    def analyze_social_sentiment(self, texts: List[str]) -> Dict[str, Any]:
        """ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¬Ù…Ø¹ÛŒ"""
        
        sentiments = []
        for text in texts[:100]:  # Ù…Ø­Ø¯ÙˆØ¯ Ø¨Ù‡ 100 Ù…ØªÙ†
            sent = self.analyze_sentiment(text)['normalized']
            sentiments.append(sent)
        
        if not sentiments:
            return {
                'average': 0.5,
                'std': 0,
                'trend': 'neutral',
                'volume': 0
            }
        
        avg_sentiment = np.mean(sentiments)
        std_sentiment = np.std(sentiments)
        
        # ØªØ´Ø®ÛŒØµ trend
        if avg_sentiment > 0.6:
            trend = 'very_positive'
        elif avg_sentiment > 0.55:
            trend = 'positive'
        elif avg_sentiment < 0.4:
            trend = 'very_negative'
        elif avg_sentiment < 0.45:
            trend = 'negative'
        else:
            trend = 'neutral'
        
        return {
            'average': float(avg_sentiment),
            'std': float(std_sentiment),
            'trend': trend,
            'volume': len(sentiments),
            'confidence': float(1 - std_sentiment)
        }
    
    # ==================== Ø°Ø®ÛŒØ±Ù‡ Ùˆ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ ====================
    
    def save_models(self):
        """Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡"""
        
        model_dir = MODELS_DIR
        model_dir.mkdir(exist_ok=True)
        
        for name, model in self.models.items():
            if name in ['dnn_classifier', 'dnn_regressor', 'lstm', 'cnn']:
                # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Keras
                try:
                    model.save(str(model_dir / f"{name}.h5"))
                except:
                    pass
            elif name == 'prophet':
                # Ø°Ø®ÛŒØ±Ù‡ Prophet
                try:
                    with open(model_dir / f"{name}.json", 'w') as f:
                        f.write(model_to_json(model))
                except:
                    pass
            else:
                # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ scikit-learn
                try:
                    with open(model_dir / f"{name}.pkl", 'wb') as f:
                        pickle.dump(model, f)
                except:
                    pass
        
        logger.info("ğŸ’¾ Models saved successfully")
    
    def load_models(self):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡"""
        
        model_dir = MODELS_DIR
        
        if not model_dir.exists():
            return
        
        for model_file in model_dir.glob("*.pkl"):
            try:
                name = model_file.stem
                with open(model_file, 'rb') as f:
                    self.models[name] = pickle.load(f)
                logger.info(f"ğŸ“¥ Loaded model: {name}")
            except Exception as e:
                logger.error(f"Error loading {model_file}: {e}")
        
        for model_file in model_dir.glob("*.h5"):
            try:
                name = model_file.stem
                self.models[name] = keras.models.load_model(str(model_file))
                logger.info(f"ğŸ“¥ Loaded Keras model: {name}")
            except Exception as e:
                logger.error(f"Error loading {model_file}: {e}")
        
        for model_file in model_dir.glob("*.json"):
            if model_file.stem == 'prophet':
                try:
                    with open(model_file, 'r') as f:
                        self.models['prophet'] = model_from_json(f.read())
                    logger.info("ğŸ“¥ Loaded Prophet model")
                except:
                    pass
    
    # ==================== APIÙ‡Ø§ÛŒ Ø®Ø§Ø±Ø¬ÛŒ ====================
    
    async def fetch_external_data(self, url: str, params: Dict = None) -> Optional[Dict]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡ Ø§Ø² APIÙ‡Ø§ÛŒ Ø®Ø§Ø±Ø¬ÛŒ"""
        
        async with aiohttp.ClientSession() as session:
            try:
                async with session.get(url, params=params, timeout=10) as response:
                    if response.status == 200:
                        return await response.json()
                    else:
                        logger.warning(f"API error {response.status}: {url}")
                        return None
            except Exception as e:
                logger.error(f"Error fetching {url}: {e}")
                return None
    
    async def get_crypto_news_sentiment(self, symbol: str) -> Dict[str, Any]:
        """Ø¯Ø±ÛŒØ§ÙØª Ùˆ ØªØ­Ù„ÛŒÙ„ Ø§Ø®Ø¨Ø§Ø± Ú©Ø±ÛŒÙ¾ØªÙˆ"""
        
        url = "https://newsapi.org/v2/everything"
        params = {
            'q': f'cryptocurrency {symbol}',
            'apiKey': NEWS_API_KEY,
            'language': 'en',
            'sortBy': 'publishedAt',
            'pageSize': 10
        }
        
        data = await self.fetch_external_data(url, params)
        
        if not data or 'articles' not in data:
            return {'sentiment': 0.5, 'articles': []}
        
        articles = data['articles'][:5]  # 5 Ø®Ø¨Ø± Ø¢Ø®Ø±
        sentiments = []
        
        for article in articles:
            title = article.get('title', '')
            description = article.get('description', '') or ''
            text = f"{title} {description}"
            
            sentiment = self.analyze_sentiment(text)['normalized']
            sentiments.append(sentiment)
        
        avg_sentiment = np.mean(sentiments) if sentiments else 0.5
        
        return {
            'sentiment': float(avg_sentiment),
            'article_count': len(articles),
            'articles': articles[:3]  # Û³ Ø®Ø¨Ø± Ø§ÙˆÙ„
        }
    
    # ==================== Ú¯Ø²Ø§Ø±Ø´ Ùˆ Ø¢Ù…Ø§Ø± ====================
    
    def get_stats(self) -> Dict[str, Any]:
        """Ú¯Ø±ÙØªÙ† Ø¢Ù…Ø§Ø± Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ"""
        
        return {
            'total_predictions': self.stats['total_predictions'],
            'accuracy': f"{self.stats['accuracy']*100:.1f}%",
            'learned_patterns': self.stats['learned_patterns'],
            'active_models': self.stats['active_models'],
            'memory_size': len(self.learning_memory),
            'pattern_memory': len(self.pattern_memory),
            'last_training': self.stats['last_training'].isoformat() if self.stats['last_training'] else 'Never',
            'confidence_threshold': self.confidence_threshold
        }
    
    def get_model_performance(self) -> Dict[str, float]:
        """Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù‡Ø± Ù…Ø¯Ù„"""
        
        performance = {}
        
        # Ø§ÛŒÙ†Ø¬Ø§ Ù…ÛŒâ€ŒØªÙˆÙ†Ù‡ cross-validation Ø§Ù†Ø¬Ø§Ù… Ø¨Ø¯Ù‡
        for name in self.models:
            performance[name] = 0.7  # placeholder
        
        return performance
